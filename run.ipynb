{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynJldC_eE9zg",
        "outputId": "1d32114e-f3dc-4399-da45-d52a88c9c8ef"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "from utils import generate_data\n",
        "from data import EncoderDataset\n",
        "from TeacherModel import DeepSupervisionNet\n",
        "from model import LearnerModel, WaypointDecoder\n",
        "from loss import KnowledgeDistillationLoss, WaypointMSE\n",
        "\n",
        "TEXTURE_PATH = \"./textures/texture.jpg\" # texture of the line drawn in simulation\n",
        "REAL_IMAGE_PATH = \"./real_images/\" # real images to augment\n",
        "\n",
        "TRAIN_DATA_PATH = \"./raw_data/train/\"\n",
        "VAL_DATA_PATH = \"./raw_data/val/\"\n",
        "FINETUNE_DATA_PATH = \"./raw_data/finetune/\"\n",
        "TEST_DATA_PATH = \"./raw_data/test/\"\n",
        "\n",
        "TRAIN_DATASET_SIZE = 512\n",
        "VAL_DATASET_SIZE = 128\n",
        "FINETUNE_DATASET_SIZE = 256\n",
        "TEST_DATASET_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "L2EpWCEfE9zk"
      },
      "outputs": [],
      "source": [
        "# Make sure the directories exist\n",
        "directories = [TRAIN_DATA_PATH, VAL_DATA_PATH, FINETUNE_DATA_PATH, TEST_DATA_PATH]\n",
        "\n",
        "for path in directories:\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    image_path = os.path.join(path, 'images')\n",
        "    if not os.path.exists(image_path):\n",
        "        os.makedirs(image_path)\n",
        "    mask_path = os.path.join(path, 'masks')\n",
        "    if not os.path.exists(mask_path):\n",
        "        os.makedirs(mask_path)\n",
        "    waypoint_path = os.path.join(path, 'labels')\n",
        "    if not os.path.exists(waypoint_path):\n",
        "        os.makedirs(waypoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRTHvaXHE9zn"
      },
      "outputs": [],
      "source": [
        "### GENERATE DATA ### (skip if data is already generated)\n",
        "\n",
        "# generate train dataset\n",
        "generate_data(line_path=TRAIN_DATA_PATH,\n",
        "              real_path=REAL_IMAGE_PATH,\n",
        "              texture_path=TEXTURE_PATH,\n",
        "              line_dataset_size=TRAIN_DATASET_SIZE)\n",
        "\n",
        "# generate val dataset\n",
        "generate_data(line_path=VAL_DATA_PATH,\n",
        "              real_path=REAL_IMAGE_PATH,\n",
        "              texture_path=TEXTURE_PATH,\n",
        "              line_dataset_size=VAL_DATASET_SIZE)\n",
        "\n",
        "# generate finetune dataset\n",
        "generate_data(line_path=FINETUNE_DATA_PATH,\n",
        "              real_path=REAL_IMAGE_PATH,\n",
        "              texture_path=TEXTURE_PATH,\n",
        "              line_dataset_size=FINETUNE_DATASET_SIZE)\n",
        "\n",
        "# generate test dataset\n",
        "generate_data(line_path=TEST_DATA_PATH,\n",
        "              real_path=REAL_IMAGE_PATH,\n",
        "              texture_path=TEXTURE_PATH,\n",
        "              line_dataset_size=TEST_DATASET_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNWZ9xfpE9zo",
        "outputId": "46e31d67-d0f4-4534-eb97-adc8e6e157b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "### SET DEVICE ###\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WfIMPu3gE9zp"
      },
      "outputs": [],
      "source": [
        "### DECLARE DATASETS AND DATALOADERS ###\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = EncoderDataset(TRAIN_DATA_PATH, dataset_size=TRAIN_DATASET_SIZE)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataset = EncoderDataset(VAL_DATA_PATH, dataset_size=VAL_DATASET_SIZE)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "finetune_dataset = EncoderDataset(FINETUNE_DATA_PATH, dataset_size=FINETUNE_DATASET_SIZE)\n",
        "finetune_loader = DataLoader(finetune_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = EncoderDataset(TEST_DATA_PATH, dataset_size=TEST_DATASET_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SJSQCuFE9zr",
        "outputId": "b11c71a6-3aa3-4077-d50f-ac275e74f8ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [10/16], Loss: 2.1210\n",
            "Epoch [1/20], Loss: 1.7373\n",
            "Epoch [2/20], Step [10/16], Loss: 0.8998\n",
            "Epoch [2/20], Loss: 0.8352\n",
            "Epoch [3/20], Step [10/16], Loss: 0.6494\n",
            "Epoch [3/20], Loss: 0.6272\n",
            "Saved checkpoint at epoch 3\n",
            "Epoch [4/20], Step [10/16], Loss: 0.8809\n",
            "Epoch [4/20], Loss: 0.8124\n",
            "Epoch [5/20], Step [10/16], Loss: 0.6112\n",
            "Epoch [5/20], Loss: 0.5960\n",
            "Epoch [6/20], Step [10/16], Loss: 0.5141\n",
            "Epoch [6/20], Loss: 0.5042\n",
            "Saved checkpoint at epoch 6\n",
            "Epoch [7/20], Step [10/16], Loss: 0.4712\n",
            "Epoch [7/20], Loss: 0.4608\n",
            "Epoch [8/20], Step [10/16], Loss: 0.5284\n",
            "Epoch [8/20], Loss: 0.5445\n",
            "Epoch [9/20], Step [10/16], Loss: 0.4848\n",
            "Epoch [9/20], Loss: 0.4732\n",
            "Saved checkpoint at epoch 9\n",
            "Epoch [10/20], Step [10/16], Loss: 0.4267\n",
            "Epoch [10/20], Loss: 0.4383\n",
            "Epoch [11/20], Step [10/16], Loss: 0.3687\n",
            "Epoch [11/20], Loss: 0.3644\n",
            "Epoch [12/20], Step [10/16], Loss: 0.3801\n",
            "Epoch [12/20], Loss: 0.3734\n",
            "Saved checkpoint at epoch 12\n",
            "Epoch [13/20], Step [10/16], Loss: 0.3232\n",
            "Epoch [13/20], Loss: 0.3231\n",
            "Epoch [14/20], Step [10/16], Loss: 0.3021\n",
            "Epoch [14/20], Loss: 0.2867\n",
            "Epoch [15/20], Step [10/16], Loss: 0.2826\n",
            "Epoch [15/20], Loss: 0.2877\n",
            "Saved checkpoint at epoch 15\n",
            "Epoch [16/20], Step [10/16], Loss: 0.3417\n",
            "Epoch [16/20], Loss: 0.3289\n",
            "Epoch [17/20], Step [10/16], Loss: 0.2778\n",
            "Epoch [17/20], Loss: 0.2922\n",
            "Epoch [18/20], Step [10/16], Loss: 0.2833\n",
            "Epoch [18/20], Loss: 0.2725\n",
            "Saved checkpoint at epoch 18\n",
            "Epoch [19/20], Step [10/16], Loss: 0.2730\n",
            "Epoch [19/20], Loss: 0.2631\n",
            "Epoch [20/20], Step [10/16], Loss: 0.2359\n",
            "Epoch [20/20], Loss: 0.2319\n",
            "Saved final model\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "### TEACHER MODEL TRAINING ###\n",
        "\n",
        "# Enable anomaly detection\n",
        "torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "# Initialize DeepSupervisionNet\n",
        "teacher_model = DeepSupervisionNet(batch_norm=True).to(device)\n",
        "teacher_model.train()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary segmentation\n",
        "optimizer = optim.Adam(teacher_model.parameters(), lr=0.003)\n",
        "\n",
        "# Directory to save the model\n",
        "save_dir = './saved_models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Check if a saved model exists and load it\n",
        "if os.path.exists(os.path.join(save_dir, 'DeepSupervisionNet.pth')):\n",
        "    teacher_model.load_state_dict(torch.load(os.path.join(save_dir, 'DeepSupervisionNet.pth')))\n",
        "    print(\"Loaded saved model.\")\n",
        "\n",
        "# Training loop\n",
        "epochs = 20\n",
        "checkpoint_epochs = [3, 6, 9, 12, 15, 18]  # Epochs at which to save checkpoints\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        inputs = batch['image'].float().to(device)  # Convert to float and move to device\n",
        "        labels = batch['mask'].float().to(device) / 255.0  # Normalize, convert to float, and move to device\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        # Forward pass with intermediate outputs\n",
        "        intermediate_outputs = teacher_model(inputs, intermediate_outputs=True)\n",
        "\n",
        "        # Compute loss for each intermediate output and the final output\n",
        "        losses = [criterion(output, labels) for output in intermediate_outputs[:5]]\n",
        "\n",
        "        # Sum up the losses\n",
        "        loss = sum(losses)\n",
        "\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print statistics\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / (i + 1):.4f}\")\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint if the epoch is in checkpoint_epochs\n",
        "    if (epoch + 1) in checkpoint_epochs:\n",
        "        checkpoint_name = f'DeepSupervisionNet_epoch_{epoch + 1}.pth'\n",
        "        torch.save(teacher_model.state_dict(), os.path.join(save_dir, checkpoint_name))\n",
        "        print(f\"Saved checkpoint at epoch {epoch + 1}\")\n",
        "\n",
        "# Save the final model\n",
        "final_model_name = 'DeepSupervisionNet_final.pth'\n",
        "torch.save(teacher_model.state_dict(), os.path.join(save_dir, final_model_name))\n",
        "print(f\"Saved final model\")\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emuKtwVoE9zu",
        "outputId": "80fc00c6-0265-4a29-d176-e6abfe626e15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### LOAD TRAINED TEACHER MODEL CHECKPOINT ###\n",
        "teacher_model = DeepSupervisionNet(batch_norm=True).to(device)\n",
        "teacher_model.load_state_dict(torch.load('./saved_models/DeepSupervisionNet_final.pth', map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7t6EdE4E9zw",
        "outputId": "b9e8d858-a43c-4907-8aa4-95f058c636cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [10/16], Loss: 2263163.5469\n",
            "Epoch [1/20], Loss: 1700536.1875\n",
            "Epoch [2/20], Step [10/16], Loss: 364690.0547\n",
            "Epoch [2/20], Loss: 328378.2500\n",
            "Epoch [3/20], Step [10/16], Loss: 234467.5453\n",
            "Epoch [3/20], Loss: 212658.4844\n",
            "Saved checkpoint at epoch 3\n",
            "Epoch [4/20], Step [10/16], Loss: 152704.3680\n",
            "Epoch [4/20], Loss: 146473.6406\n",
            "Epoch [5/20], Step [10/16], Loss: 121983.3398\n",
            "Epoch [5/20], Loss: 116996.8872\n",
            "Epoch [6/20], Step [10/16], Loss: 102874.9750\n",
            "Epoch [6/20], Loss: 98942.0854\n",
            "Saved checkpoint at epoch 6\n",
            "Epoch [7/20], Step [10/16], Loss: 90067.7695\n",
            "Epoch [7/20], Loss: 83704.4241\n",
            "Epoch [8/20], Step [10/16], Loss: 72839.6719\n",
            "Epoch [8/20], Loss: 73837.0403\n",
            "Epoch [9/20], Step [10/16], Loss: 75965.0285\n",
            "Epoch [9/20], Loss: 79938.5479\n",
            "Saved checkpoint at epoch 9\n",
            "Epoch [10/20], Step [10/16], Loss: 77515.7992\n",
            "Epoch [10/20], Loss: 81572.8052\n",
            "Epoch [11/20], Step [10/16], Loss: 102647.3914\n",
            "Epoch [11/20], Loss: 88983.5728\n",
            "Epoch [12/20], Step [10/16], Loss: 60371.5367\n",
            "Epoch [12/20], Loss: 58499.4744\n",
            "Saved checkpoint at epoch 12\n",
            "Epoch [13/20], Step [10/16], Loss: 67829.7895\n",
            "Epoch [13/20], Loss: 64747.0117\n",
            "Epoch [14/20], Step [10/16], Loss: 52432.2543\n",
            "Epoch [14/20], Loss: 48915.8938\n",
            "Epoch [15/20], Step [10/16], Loss: 48422.8781\n",
            "Epoch [15/20], Loss: 45887.8605\n",
            "Saved checkpoint at epoch 15\n",
            "Epoch [16/20], Step [10/16], Loss: 40549.4062\n",
            "Epoch [16/20], Loss: 41534.6074\n",
            "Epoch [17/20], Step [10/16], Loss: 47926.5566\n",
            "Epoch [17/20], Loss: 57001.3191\n",
            "Epoch [18/20], Step [10/16], Loss: 63015.1332\n",
            "Epoch [18/20], Loss: 71995.4601\n",
            "Saved checkpoint at epoch 18\n",
            "Epoch [19/20], Step [10/16], Loss: 63535.5242\n",
            "Epoch [19/20], Loss: 62227.9806\n",
            "Epoch [20/20], Step [10/16], Loss: 48452.3533\n",
            "Epoch [20/20], Loss: 47994.3640\n",
            "Saved final model\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "### KNOWLEDGE DISTILATION TRAINING ###\n",
        "\n",
        "# Set teacher model to eval mode\n",
        "teacher_model.eval()\n",
        "\n",
        "# Initialize DeepSupervisionNet\n",
        "learner_model = LearnerModel().to(device)\n",
        "learner_model.train()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = KnowledgeDistillationLoss()  # Binary Cross-Entropy Loss for binary segmentation\n",
        "optimizer = optim.Adam(learner_model.parameters(), lr=0.003)\n",
        "\n",
        "# Directory to save the model\n",
        "save_dir = './saved_models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Kknowledge Distilation Training Loop\n",
        "epochs = 20\n",
        "checkpoint_epochs = [3, 6, 9, 12, 15, 18]  # Epochs at which to save checkpoints\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i , batch in enumerate(train_loader):\n",
        "        inputs = batch['image'].float().to(device)  # Convert to float and move to device\n",
        "        labels = batch['mask'].float().to(device) / 255.0  # Normalize, convert to float, and move to device\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            intermediate_outputs = teacher_model(inputs, intermediate_outputs=True)\n",
        "            teacher_outputs = intermediate_outputs[5:] # modified model to output intermediate layers for training the student model. only need 'x1', 'x2' and 'x4'\n",
        "\n",
        "        teacher_outputs.pop(2) # discard the third output\n",
        "\n",
        "        learner_outputs, _ = learner_model(inputs, intermediate_outputs=True)\n",
        "\n",
        "        # Compute the loss over each of the intermediate outputs from the teacher and student models\n",
        "        losses = []\n",
        "        for learner_output, teacher_output in zip(learner_outputs, teacher_outputs):\n",
        "            # print(f'Shape of learner output: {learner_output.shape}')\n",
        "            # print(f'Shape of teacher output: {teacher_output.shape}')\n",
        "            losses.append(criterion(learner_output, teacher_output))\n",
        "\n",
        "        # losses = [criterion(learner_output, teacher_output) for learner_output, teacher_output in zip(learner_outputs, teacher_outputs)]\n",
        "\n",
        "        # Sum up the losses\n",
        "        loss = sum(losses)\n",
        "\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print statistics\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / (i + 1):.4f}\")\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint if the epoch is in checkpoint_epochs\n",
        "    if (epoch + 1) in checkpoint_epochs:\n",
        "        checkpoint_name = f'LearnerModel_epoch_{epoch + 1}.pth'\n",
        "        torch.save(learner_model.state_dict(), os.path.join(save_dir, checkpoint_name))\n",
        "        print(f\"Saved checkpoint at epoch {epoch + 1}\")\n",
        "\n",
        "# Save the final model\n",
        "final_model_name = 'LearnerModel_final.pth'\n",
        "torch.save(learner_model.state_dict(), os.path.join(save_dir, final_model_name))\n",
        "print(f\"Saved final model\")\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6a7kShjE9zy"
      },
      "outputs": [],
      "source": [
        "### LOAD TRAINED STUDENT MODEL CHECKPOINT ###\n",
        "prune_learner_model = LearnerModel().to(device)\n",
        "prune_learner_model.load_state_dict(torch.load('./saved_models/LearnerModel_final.pth', map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CuOQwhQE9zz"
      },
      "outputs": [],
      "source": [
        "### STUDENT MODEL PRUNING AND FINE-TUNE TRAINING ###\n",
        "\n",
        "# Set teacher model to eval mode\n",
        "teacher_model.eval()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = KnowledgeDistillationLoss()  # Binary Cross-Entropy Loss for binary segmentation\n",
        "optimizer = optim.Adam(prune_learner_model.parameters(), lr=0.001)\n",
        "\n",
        "# Directory to save the model\n",
        "save_dir = './saved_models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "epochs = 10 # change epochs here\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    feat_across_batches = []\n",
        "    prune_learner_model.eval()\n",
        "    for batch in val_loader:\n",
        "        prune_learner_model.to(device)\n",
        "        inputs = batch['image'].float().to(device)\n",
        "        with torch.no_grad():\n",
        "            _, features = prune_learner_model(inputs, intermediate_outputs=True)\n",
        "\n",
        "        if feat_across_batches == []:\n",
        "            feat_across_batches = features\n",
        "        else:\n",
        "            for i in range(len(features)):\n",
        "                batch_concat = feat_across_batches[i]\n",
        "                feat_across_batches[i] = torch.cat((batch_concat, features[i]), 0)\n",
        "\n",
        "    for i, layer_features in enumerate(feat_across_batches):\n",
        "        this_layer = []\n",
        "        num_channels = layer_features.shape[1]\n",
        "        for j in range(num_channels):\n",
        "            channel_features = layer_features[:, j, :, :]\n",
        "            zeroes = torch.sum(channel_features == 0).item()\n",
        "            num_elements = channel_features.numel()\n",
        "            channel_apz = zeroes / num_elements\n",
        "            this_layer.append(channel_apz)\n",
        "\n",
        "        # prune layer\n",
        "        layer_mean = np.array(this_layer).mean()\n",
        "        layer_std = np.array(this_layer).std() * 1.2\n",
        "\n",
        "        channel_indexes = []\n",
        "        for k in range(len(this_layer)):\n",
        "            if (this_layer[k] < (layer_mean - layer_std)) or (this_layer[k] > (layer_mean + layer_std)):\n",
        "                channel_indexes.append(k)\n",
        "\n",
        "        model_layers, next_layers = prune_learner_model.get_prunable_layers()\n",
        "        model_layer = copy.deepcopy(model_layers[i])\n",
        "        next_layer = copy.deepcopy(next_layers[i])\n",
        "\n",
        "        for idx in channel_indexes[::-1]:\n",
        "            model_layer.weight = nn.Parameter(torch.cat([model_layer.weight[:idx, :, :, :],\n",
        "                                           model_layer.weight[idx+1:, :, :, :]], dim=0))\n",
        "            model_layer.bias = nn.Parameter(torch.cat([model_layer.bias[:idx], model_layer.bias[idx+1:]], dim=0))\n",
        "            next_layer.weight = nn.Parameter(torch.cat([next_layer.weight[:, :idx, :, :],\n",
        "                                           next_layer.weight[:, idx+1:, :, :]], dim=1))\n",
        "\n",
        "        prune_learner_model.set_pruned_layers(i, model_layer, next_layer, model_layer.weight.shape[0])\n",
        "\n",
        "    # post-prune fine-tuning\n",
        "    prune_learner_model.to(device)\n",
        "    prune_learner_model.train()\n",
        "    for i , batch in enumerate(finetune_loader):\n",
        "        inputs = batch['image'].float().to(device)  # Convert to float and move to device\n",
        "        labels = batch['mask'].float().to(device) / 255.0  # Normalize, convert to float, and move to device\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            intermediate_outputs = teacher_model(inputs, intermediate_outputs=True)\n",
        "            teacher_outputs = intermediate_outputs[5:] # modified model to output intermediate layers for training the student model. only need 'x1', 'x2' and 'x4'\n",
        "\n",
        "        teacher_outputs.pop(2) # discard the third output\n",
        "        learner_outputs, _ = prune_learner_model(inputs, intermediate_outputs=True)\n",
        "\n",
        "        # Compute the loss over each of the intermediate outputs from the teacher and student models\n",
        "        losses = []\n",
        "        for learner_output, teacher_output in zip(learner_outputs, teacher_outputs):\n",
        "            losses.append(criterion(learner_output, teacher_output))\n",
        "\n",
        "        loss = sum(losses)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint if the epoch is in checkpoint_epochs\n",
        "    if (epoch + 1) in checkpoint_epochs:\n",
        "        checkpoint_name = f'LearnerModelPruned_epoch_{epoch + 1}.pth'\n",
        "        torch.save(prune_learner_model.state_dict(), os.path.join(save_dir, checkpoint_name))\n",
        "        print(f\"Saved checkpoint at epoch {epoch + 1}\")\n",
        "\n",
        "# Save the final model\n",
        "final_model_name = 'LearnerModelPruned_final.pth'\n",
        "torch.save(prune_learner_model.state_dict(), os.path.join(save_dir, final_model_name))\n",
        "print(f\"Saved final model\")\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "KTdFWejoXjYH"
      },
      "outputs": [],
      "source": [
        "# Special code to save pruned model\n",
        "res1_conv1 = prune_learner_model.resblock1.conv1.bias.shape[0]\n",
        "res1_conv2 = prune_learner_model.resblock1.conv2.bias.shape[0]\n",
        "res2_conv1 = prune_learner_model.resblock2.conv1.bias.shape[0]\n",
        "res2_conv2 = prune_learner_model.resblock2.conv2.bias.shape[0]\n",
        "pruned_channels = [res1_conv1, res1_conv2, res2_conv1, res2_conv2]\n",
        "\n",
        "with open('./saved_models/channel_list.txt', 'w') as f:\n",
        "    for item in pruned_channels:\n",
        "        f.write(f\"{item}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpDzXKQeE9z1"
      },
      "outputs": [],
      "source": [
        "### PLOT OUTPUTS OF ENCODER MODEL FOR VISUALISATION###\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for inference\n",
        "    for i, batch in enumerate(test_loader):\n",
        "        inputs = batch['image'].float().to(device)  # Convert to float and move to device\n",
        "        model_outputs = prune_learner_model(inputs, intermediate_outputs=False)\n",
        "\n",
        "        for out in model_outputs:\n",
        "            out = out.cpu().numpy()\n",
        "            img = np.mean(out, axis=0)\n",
        "            img = (img - img.min()) / (img.max() - img.min())\n",
        "\n",
        "            plt.figure()\n",
        "            plt.imshow(img, cmap='gray')\n",
        "            plt.axis('off')  # Hide axis\n",
        "            plt.title('Image Representation of Encoder Output')\n",
        "            plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB4v6kq8bo-s"
      },
      "outputs": [],
      "source": [
        "# Special code to load pruned model\n",
        "channel_list = []\n",
        "with open('./saved_models/channel_list.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        channel_list.append(int(line.strip()))\n",
        "\n",
        "final_encoder_model = LearnerModel(channel_list).to(device)\n",
        "final_encoder_model.load_state_dict(torch.load('./saved_models/LearnerModelPruned_final.pth', map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fwgau6TYQOMC",
        "outputId": "0e297a0e-2030-4fba-a832-81bb5f50c7ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Step [10/16], Loss: 5.4665\n",
            "Epoch [1/20], Loss: 3.4215\n",
            "Epoch [2/20], Step [10/16], Loss: 0.0034\n",
            "Epoch [2/20], Loss: 0.0024\n",
            "Epoch [3/20], Step [10/16], Loss: 0.0005\n",
            "Epoch [3/20], Loss: 0.0004\n",
            "Saved checkpoint at epoch 3\n",
            "Epoch [4/20], Step [10/16], Loss: 0.0002\n",
            "Epoch [4/20], Loss: 0.0002\n",
            "Epoch [5/20], Step [10/16], Loss: 0.0001\n",
            "Epoch [5/20], Loss: 0.0001\n",
            "Epoch [6/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [6/20], Loss: 0.0000\n",
            "Saved checkpoint at epoch 6\n",
            "Epoch [7/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [7/20], Loss: 0.0000\n",
            "Epoch [8/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [8/20], Loss: 0.0000\n",
            "Epoch [9/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [9/20], Loss: 0.0000\n",
            "Saved checkpoint at epoch 9\n",
            "Epoch [10/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [10/20], Loss: 0.0000\n",
            "Epoch [11/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [11/20], Loss: 0.0000\n",
            "Epoch [12/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [12/20], Loss: 0.0000\n",
            "Saved checkpoint at epoch 12\n",
            "Epoch [13/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [13/20], Loss: 0.0000\n",
            "Epoch [14/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [14/20], Loss: 0.0000\n",
            "Epoch [15/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [15/20], Loss: 0.0000\n",
            "Saved checkpoint at epoch 15\n",
            "Epoch [16/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [16/20], Loss: 0.0000\n",
            "Epoch [17/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [17/20], Loss: 0.0000\n",
            "Epoch [18/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [18/20], Loss: 0.0000\n",
            "Saved checkpoint at epoch 18\n",
            "Epoch [19/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [19/20], Loss: 0.0000\n",
            "Epoch [20/20], Step [10/16], Loss: 0.0000\n",
            "Epoch [20/20], Loss: 0.0000\n",
            "Saved final model\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "### WAYPOINT DECODER TRAINING ###\n",
        "final_encoder_model.eval()\n",
        "decoder_model = WaypointDecoder().to(device)\n",
        "decoder_model.train()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = WaypointMSE()  # Binary Cross-Entropy Loss for binary segmentation\n",
        "optimizer = optim.Adam(decoder_model.parameters(), lr=0.003)\n",
        "\n",
        "# Directory to save the model\n",
        "save_dir = './saved_models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Training loop\n",
        "epochs = 20\n",
        "checkpoint_epochs = [3, 6, 9, 12, 15, 18]  # Epochs at which to save checkpoints\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, batch in enumerate(train_loader):\n",
        "        inputs = batch['image'].float().to(device)  # Convert to float and move to device\n",
        "        labels = batch['label'].float().to(device) / 255.0  # Normalize, convert to float, and move to device\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        with torch.no_grad():\n",
        "            encoder_outputs = final_encoder_model(inputs, intermediate_outputs=False)\n",
        "\n",
        "        # Forward pass with intermediate outputs\n",
        "        waypoint_outputs = decoder_model(encoder_outputs)\n",
        "\n",
        "        loss = criterion(waypoint_outputs, labels)\n",
        "\n",
        "        # Backward propagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Print statistics\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / (i + 1):.4f}\")\n",
        "\n",
        "    # Average loss for the epoch\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    # Save checkpoint if the epoch is in checkpoint_epochs\n",
        "    if (epoch + 1) in checkpoint_epochs:\n",
        "        checkpoint_name = f'WaypointDecoder_epoch_{epoch + 1}.pth'\n",
        "        torch.save(decoder_model.state_dict(), os.path.join(save_dir, checkpoint_name))\n",
        "        print(f\"Saved checkpoint at epoch {epoch + 1}\")\n",
        "\n",
        "# Save the final model\n",
        "final_model_name = 'WaypointDecoder_final.pth'\n",
        "torch.save(decoder_model.state_dict(), os.path.join(save_dir, final_model_name))\n",
        "print(f\"Saved final model\")\n",
        "\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvPnI4KYgCnw",
        "outputId": "0a802d7f-25b0-4a89-de59-04a779ac1642"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# LOAD WAYPOINT DECODER\n",
        "final_decoder_model = WaypointDecoder().to(device)\n",
        "final_decoder_model.load_state_dict(torch.load('./saved_models/WaypointDecoder_final.pth', map_location=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFsae5-gZUpG",
        "outputId": "88638f1e-92c8-4489-d860-d910ba0ab173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.06151349\n"
          ]
        }
      ],
      "source": [
        "final_encoder_model.eval()\n",
        "final_decoder_model.eval()\n",
        "total_diff = 0\n",
        "with torch.no_grad():  # Disable gradient calculation for inference\n",
        "    for i, batch in enumerate(test_loader):\n",
        "        inputs = batch['image'].float().to(device)  # Convert to float and move to device\n",
        "        labels = batch['label'].float().to(device)\n",
        "        encoder_outputs = final_encoder_model(inputs)\n",
        "        decoder_outputs = final_decoder_model(encoder_outputs)\n",
        "\n",
        "        diff = ((labels - decoder_outputs)**2)/labels.shape[0]\n",
        "        total_diff += diff\n",
        "\n",
        "total_diff = total_diff.cpu().numpy()\n",
        "total_diff = np.sum(np.sum(total_diff))\n",
        "print(total_diff)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGVuFl4AzSRY"
      },
      "outputs": [],
      "source": [
        "# inference mode\n",
        "channel_list = []\n",
        "with open('channel_list.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        channel_list.append(int(line.strip()))\n",
        "\n",
        "final_encoder_model = LearnerModel(channel_list).to(device)\n",
        "final_encoder_model.load_state_dict(torch.load('./saved_models/LearnerModelPruned_final.pth', map_location=device))\n",
        "final_decoder_model = WaypointDecoder().to(device)\n",
        "final_decoder_model.load_state_dict(torch.load('./saved_models/WaypointDecoder_final.pth', map_location=device))\n",
        "\n",
        "ingestion = None # insert input image here\n",
        "\n",
        "# TODO: convert image to tensor of shape (1, 3, 200, 200)\n",
        "\n",
        "final_encoder_model.eval()\n",
        "final_decoder_model.eval()\n",
        "with torch.no_grad():\n",
        "    encoded_image = final_encoder_model(ingestion)\n",
        "    decoded_waypoint = final_decoder_model(encoded_image)\n",
        "\n",
        "x, y, _ = tuple(decoded_waypoint)\n",
        "\n",
        "# next, send (x, y) waypoint to the PID controller"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
